{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Iterable, Optional, Sequence, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import masi_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krippendorff.krippendorff import _coincidences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "РАСХОЖДЕНИЕ ИЗ -ЗА РАЗНЫХ МЕТРИК ДИСТАНЦИИ И СООТВЕТСКНЕО ПЛОЗО КОГДА МНОГО ИЗМЕРЕНИЙ СТАНОВИТСЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/questions/407453/krippendorffs-alpha-in-r-for-multi-label-annotation\n",
    "import nltk\n",
    "from nltk.metrics import agreement\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "\n",
    "\n",
    "task_data = [('coder1','Item0',frozenset(['l1','l2'])),\n",
    "('coder2','Item0',frozenset(['l1'])),\n",
    "('coder1','Item1',frozenset(['l1','l2'])),\n",
    "('coder2','Item1',frozenset(['l1','l2'])),\n",
    "('coder1','Item2',frozenset(['l1'])),\n",
    "('coder2','Item2',frozenset(['l1']))]\n",
    "\n",
    "task = AnnotationTask(distance = masi_distance)\n",
    "\n",
    "task.load_array(task_data)\n",
    "\n",
    "task.alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_agreement_within_group_nofilter(df_group):\n",
    "    worker2id = {}\n",
    "    for pair in df_group['ASSIGNMENT:worker_id'].tolist():\n",
    "        if pair not in worker2id:\n",
    "            worker2id[pair] = len(worker2id)\n",
    "    df_group['worker_idx'] = df_group['ASSIGNMENT:worker_id'].map(worker2id)\n",
    "    \n",
    "    text2ids = {}\n",
    "    for t in set(df_group['INPUT:text'].tolist()):\n",
    "        if t not in text2ids:\n",
    "            text2ids[t] = len(text2ids)\n",
    "    df_group['text_idx'] = df_group['INPUT:text'].map(text2ids)\n",
    "    ids2text = {idx:text for text,idx in text2ids.items()}\n",
    "    \n",
    "    reply_columns = [c for c in df_group.columns if 'OUTPUT' in c]\n",
    "    \n",
    "    collected_data = []\n",
    "    for i,el in df_group.iterrows():\n",
    "        coder_idx = el['worker_idx']\n",
    "        \n",
    "        item_idx = el['text_idx']\n",
    "        labels_list = []\n",
    "        for repl in reply_columns:\n",
    "#             print(repl[7:], el[repl])\n",
    "            if el[repl] == True:\n",
    "                labels_list.append(repl[7:])\n",
    "        \n",
    "        if len(labels_list) == 0: labels_list = ['none']\n",
    "#         print(labels_list)\n",
    "        collected_triplet = (coder_idx,item_idx,frozenset(labels_list))\n",
    "        collected_data.append(collected_triplet)\n",
    "        \n",
    "#         collected_texts.append(el['INPUT:text'])\n",
    "        \n",
    "#         break\n",
    "    task = AnnotationTask(distance = masi_distance)\n",
    "    \n",
    "#     print(\"collected_data\", collected_data[:10])\n",
    "\n",
    "    task.load_array(collected_data)\n",
    "\n",
    "    return task.alpha()\n",
    "\n",
    "\n",
    "def get_agreement_batch(batch_name, drop_traintest_examples = False, print_output = False,):\n",
    "    if batch_name == 'first': \n",
    "        raw_files = ['g1r.tsv','g2r.tsv','g3r.tsv']\n",
    "    elif batch_name == 'fourth':\n",
    "        raw_files = ['g1_4r.tsv','g2_4r.tsv','g3_4r.tsv']\n",
    "    elif batch_name == 'old':\n",
    "        raw_files = ['maybe_old/g1r.tsv','maybe_old/g2r.tsv']\n",
    "    \n",
    "    index2textset_notrest = {}\n",
    "    index2textset_withtrtest = {}\n",
    "    for file in raw_files:\n",
    "       \n",
    "        batch_path = \"./directly_from_toloka/\"+file\n",
    "        df_curr= pd.read_csv(batch_path, sep = '\\t')\n",
    "        \n",
    "        index2textset_withtrtest[file] = set(df_curr['INPUT:text'])\n",
    "        \n",
    "        if drop_traintest_examples == True:\n",
    "            golden_column = [c for c in df_curr.columns if 'GOLDEN' in c]\n",
    "            df_curr = df_curr[df_curr[golden_column[1]].isna()]\n",
    "        \n",
    "        index2textset_notrest[file] = set(df_curr['INPUT:text'])\n",
    "    \n",
    "    intersect_text_no_tr_test = index2textset_notrest[file]\n",
    "    engaged_text_withtrtest = index2textset_withtrtest[file]\n",
    "    for f in index2textset_notrest:\n",
    "        intersect_text_no_tr_test = intersect_text_no_tr_test.intersection(index2textset_notrest[f])\n",
    "        engaged_text_withtrtest = engaged_text_withtrtest.union(index2textset_withtrtest[f])\n",
    "        \n",
    "    if print_output == True: print(\"intersecting texts are {} \\n\".format(len(intersect_text)))\n",
    "    \n",
    "    collected_agreements = []\n",
    "    for i, file in enumerate(raw_files):\n",
    "        batch_path = \"./directly_from_toloka/\"+file\n",
    "        df_curr_common= pd.read_csv(batch_path, sep = '\\t')\n",
    "        \n",
    "        if drop_traintest_examples == True:\n",
    "            golden_column = [c for c in df_curr_common.columns if 'GOLDEN' in c]\n",
    "            df_curr_common = df_curr_common[df_curr_common[golden_column[1]].isna()]\n",
    "        \n",
    "        columns_to_merge = [c for c in df_curr_common.columns if c == 'INPUT:text' or 'OUTPUT' in c  or c =='ASSIGNMENT:worker_id']\n",
    "        df_curr_common = df_curr_common[columns_to_merge]\n",
    "        \n",
    "        current_aggr = calc_agreement_within_group_nofilter(df_curr_common)\n",
    "#         print(\"current_aggr\", current_aggr)\n",
    "        collected_agreements.append(current_aggr)\n",
    "        \n",
    "#         break\n",
    "    if print_output == True: \n",
    "        print(\"\\n\")\n",
    "        print(\"collected_agreements\", collected_agreements)\n",
    "        print(\"average agreements\", np.mean(collected_agreements))\n",
    "    return intersect_text_no_tr_test, engaged_text_withtrtest, collected_agreements#intersect_text_no_tr_test - с тренй-тест если выбрать опцию не дропать тртест\n",
    "\n",
    "    \n",
    "int_text, eng_text, afrr = get_agreement_batch('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whole_agreement_and_intersections(drop_traintest_examples, iterate_over_batches_list = ['first','fourth','old']):\n",
    "    intersected_texts_from_batches = set()\n",
    "    all_unique_texts_from_batches =  set()\n",
    "    toloka_agreements_list = []\n",
    "    for batch in iterate_over_batches_list:#\n",
    "        intersect_texts, all_unique_texts_set, aggr_list = get_agreement_batch(batch, drop_traintest_examples = drop_traintest_examples)\n",
    "        intersected_texts_from_batches = intersected_texts_from_batches.union(intersect_texts)\n",
    "        all_unique_texts_from_batches = all_unique_texts_from_batches.union(all_unique_texts_set)\n",
    "        print(\"{} batch agreement list - {}\".format(batch, aggr_list))\n",
    "        toloka_agreements_list.extend(aggr_list)\n",
    "    return intersected_texts_from_batches,all_unique_texts_from_batches, np.mean(toloka_agreements_list)\n",
    "common_texts_whole_toloka_notrtes, all_engaged_texts, whole_toloka_agreement = get_whole_agreement_and_intersections(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.10661798648012055, 0.6364950196276913, 0.3874856997131322] - без трейн-тест семплов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_texts_whole_toloka_notrtes), len(all_engaged_texts), whole_toloka_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts_whole_toloka_withtrtest,  all_engaged_texts, whole_toloka_agreement = get_whole_agreement_and_intersections(drop_traintest_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_texts_whole_toloka_withtrtest), len(all_engaged_texts), whole_toloka_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_texts_whole_toloka_withtrtest), len(all_engaged_texts), whole_toloka_agreement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Не очень понятно какому расчету больше доверять - с учетом трейн-тест семплов или нет. Наличие чрезмерного количества ответов на трейн-тест семплы мождет перекручивать значение. Необходимо проверить насколько часто в реально выложенных от толоки семпах встречались семплы из тренировочных сетов. Сделаем это в след секции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверяем пересечение текстов фактических батчей с сырой толоки с текстами заявленными в спарсенном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_len(text):\n",
    "    if len(text.split()) > 5 and len (text)< 250:\n",
    "        return True\n",
    "    return None\n",
    "def is_mostly_russian(text):\n",
    "    text = str(text)\n",
    "    russian_letters_count = len(re.findall(\"[а-яА-Я]\",text))\n",
    "    eng_letters_count = len(re.findall(\"[a-zA-Z]\",text))\n",
    "    if russian_letters_count > eng_letters_count:\n",
    "        return True\n",
    "    return None\n",
    "def depersonalize(text):\n",
    "    text = str(text)\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text= re.sub(url_regex, \"url\", text)\n",
    "    text = re.sub(\"id[\\d]*\",'',text)\n",
    "    text = re.sub(\"> ?\",'',text)\n",
    "    text = re.sub(\"@[\\w]*\",'',text)\n",
    "#     text = re.sub(\"\\+[\\d]*\", \"\", text)\n",
    "    text = re.sub(\"[\\d]+\", \"NUMBER\", text)\n",
    "    text = re.sub(\">>\", \"\", text)\n",
    "#     text = re.sub(\"[\\d]{3,100}\", \"\", text)\n",
    "    text = re.sub(\"[\\t|\\n|\\r]\", \"\", text)\n",
    "    text = re.sub(\"[(OP)|url|nickname|phone_number]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive = pd.read_csv(\"sensitive_topics_vs_source.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive_toloka = df_sensitive[df_sensitive['source'] == 'toloka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensitive_toloka_texts_set = set(df_sensitive_toloka['text'])\n",
    "len(df_sensitive_toloka_texts_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### если дропать трейн тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_texts_whole_toloka_notrtes, all_engaged_texts, whole_toloka_agreement = get_whole_agreement_and_intersections(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts_whole_toloka_prep = set([depersonalize(t) for t in common_texts_whole_toloka_notrtes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & common_texts_whole_toloka_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & common_texts_whole_toloka_prep)/len(df_sensitive_toloka_texts_set)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "если использовать тексты только общие внутри каждого батча с учетом дропа трейн-тест то покрытие толоки довольно низкое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### если НЕ дропать трейн тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts_whole_toloka_withtrtest,  all_engaged_texts, whole_toloka_agreement = get_whole_agreement_and_intersections(drop_traintest_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts_whole_toloka_withtrtest_prep = set([depersonalize(t) for t in common_texts_whole_toloka_withtrtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & common_texts_whole_toloka_withtrtest_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & common_texts_whole_toloka_withtrtest_prep)/len(df_sensitive_toloka_texts_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6331054485187487"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "если использовать тексты только общие внутри каждого батча БЕЗ дропа трейн-тест то покрытие толоки тоже довольно низкое"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_engaged_texts_prep = set([depersonalize(t) for t in all_engaged_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & all_engaged_texts_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & all_engaged_texts_prep)/len(df_sensitive_toloka_texts_set)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Получается что в оснвоном все то что используется в итоговом датасете как толока включает в себя не только пересекающиеся семплы внутри батчей, но и еще такие семплы, которые по каким-то причинам 9скоре всего просто ошибочно) не вошли в ту или иную группу внутри какого-то бачта. Это не оч хорошо, так как такого рода семплы не размечены на предмет топкиа внутри одной группы но не размечены на предмет топика внутри другой. Однако учитывая тот факт, что семплы как правило предварительно каким-либо образом подбирались, можно остаивть все как есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts_whole_toloka_withtrtest,  all_engaged_texts, whole_toloka_agreement = get_whole_agreement_and_intersections(drop_traintest_examples = False,\n",
    "                                                                                                                        iterate_over_batches_list = ['first','fourth'])\n",
    "\n",
    "all_engaged_texts_prep = set([depersonalize(t) for t in all_engaged_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sensitive_toloka_texts_set & all_engaged_texts_prep)/len(df_sensitive_toloka_texts_set)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Старый батч включает в себя приличное кол-во текстов из выложенного датасета, то есть если его и скидывать, то толкьо в качеств прунинга итогогво датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пытаемся попрунить толокерский датасет для увеличения согласованности"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Думаю, что с учетом выясненной существенной применяемости тренй-тест семплов в итоговом датасете следует не ибавляться от них, но те голоса, которые явно делают поблажку собранным данным (там где сотни голосов  примененных к одному семплу) - срезать количество этих голосов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sens = pd.read_csv(\"sensitive_topics_vs_source.csv\")\n",
    "toloka_trtes_set = set(df_sens[df_sens['source']=='toloka_trtest']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krippendorf_with_filtering(collected_data,columns,ids2text, admit_multi, min_votes, max_reply_threshold):\n",
    "    count_dict = {i:{c:0 for c in columns} for i in range(len(ids2text))}\n",
    "\n",
    "    for _ ,item_idx,label_set in collected_data:\n",
    "        for label in label_set:\n",
    "            count_dict[item_idx][label] += 1\n",
    "            \n",
    "    value_counts_real = []\n",
    "    value_counts_pruned = []\n",
    "    value_counts_pruned_data = []\n",
    "    \n",
    "    value_counts_strict = []\n",
    "    value_counts_strict_data = []\n",
    "    \n",
    "    for task_idx in range(len(ids2text)):\n",
    "        current_count_element_real = [0] * len(columns) \n",
    "        current_count_element_pruned = [0] * len(columns)\n",
    "        for idx, lbl in enumerate(columns):\n",
    "            if count_dict[task_idx][lbl] > max_reply_threshold:\n",
    "                print(count_dict[task_idx][lbl],task_idx,  lbl)\n",
    "            current_count_element_pruned[idx] = min(max_reply_threshold, count_dict[task_idx][lbl])\n",
    "            value_counts_pruned_data.append(ids2text[task_idx])\n",
    "                \n",
    "            current_count_element_real[idx] = count_dict[task_idx][lbl]\n",
    "           \n",
    "        value_counts_real.append(current_count_element_real)\n",
    "        value_counts_pruned.append(current_count_element_pruned)\n",
    "        \n",
    "        is_strict = meets_strict_req(current_count_element_pruned,admit_multi, min_votes)\n",
    "        if is_strict == True:\n",
    "            value_counts_strict.append(current_count_element_pruned)\n",
    "            value_counts_strict_data.append(ids2text[task_idx])\n",
    "     \n",
    "    aplha_no_strict_real  = krippendorff.alpha(value_counts=np.array(value_counts_real),level_of_measurement='nominal')\n",
    "    aplha_no_strict_pruned = krippendorff.alpha(value_counts=np.array(value_counts_pruned),level_of_measurement='nominal')\n",
    "    alpha_strict = krippendorff.alpha(value_counts=np.array(value_counts_strict),level_of_measurement='nominal')\n",
    "    \n",
    "    unique_texts_from_all_val_counts = set(value_counts_pruned_data)\n",
    "    unique_texts_from_strict_val_counts = set(value_counts_strict_data)\n",
    "    \n",
    "    return (aplha_no_strict_real,aplha_no_strict_pruned, alpha_strict), unique_texts_from_all_val_counts, unique_texts_from_strict_val_counts\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "before dropping trtest ... 12705\n",
      "after dropping trtest ... 10006\n",
      "before dropping low conf ... 10006\n",
      "after dropping low conf ... 7668\n",
      "====================================================================================================\n",
      "before dropping trtest ... 15009\n",
      "after dropping trtest ... 12250\n",
      "before dropping low conf ... 12250\n",
      "after dropping low conf ... 8540\n",
      "====================================================================================================\n",
      "before dropping trtest ... 16934\n",
      "after dropping trtest ... 13810\n",
      "before dropping low conf ... 13810\n",
      "after dropping low conf ... 8641\n"
     ]
    }
   ],
   "source": [
    "def calc_agreement_within_group_filter(df_group):\n",
    "    worker2id = {}\n",
    "    for pair in df_group['ASSIGNMENT:worker_id'].tolist():\n",
    "        if pair not in worker2id:\n",
    "            worker2id[pair] = len(worker2id)\n",
    "    df_group['worker_idx'] = df_group['ASSIGNMENT:worker_id'].map(worker2id)\n",
    "    \n",
    "    text2ids = {}\n",
    "    for t in set(df_group['INPUT:text'].tolist()):\n",
    "        if t not in text2ids:\n",
    "            text2ids[t] = len(text2ids)\n",
    "    df_group['text_idx'] = df_group['INPUT:text'].map(text2ids)\n",
    "    ids2text = {idx:text for text,idx in text2ids.items()}\n",
    "    \n",
    "    reply_columns = [c for c in df_group.columns if 'OUTPUT' in c]\n",
    "    \n",
    "    collected_data = []\n",
    "    item_idx_dict = {}\n",
    "    \n",
    "    for i,el in df_group.iterrows():\n",
    "        coder_idx = el['worker_idx']\n",
    "        \n",
    "        item_idx = el['text_idx']\n",
    "        \n",
    "        if item_idx in item_idx_dict:\n",
    "            item_idx_dict[item_idx] += 1\n",
    "        else:\n",
    "            item_idx_dict[item_idx] = 1\n",
    "            \n",
    "        labels_list = []\n",
    "        for repl in reply_columns:\n",
    "            if el[repl] == True:\n",
    "                labels_list.append(repl[7:])\n",
    "\n",
    "        if len(labels_list) == 0: labels_list = ['none']\n",
    "        collected_triplet = (coder_idx,item_idx,frozenset(labels_list))\n",
    "        collected_data.append(collected_triplet)\n",
    "        \n",
    "    task = AnnotationTask(distance = masi_distance)\n",
    "    \n",
    "\n",
    "    task.load_array(collected_data)\n",
    "    \n",
    "    return task.alpha() #, collected_data, [r[7:] for r in reply_columns], ids2text\n",
    "\n",
    "\n",
    "def meets_strict_req(element_list,admit_multi, min_votes):\n",
    "    admitable_multichoice_count = admit_multi\n",
    "    multichoice_list = [el for el in element_list if el > min_votes] \n",
    "    if len(multichoice_list) <= admitable_multichoice_count:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_high_conf_texts(df_hc, thrsh):\n",
    "    confidnece_columns = []\n",
    "    for c in df_hc.columns:\n",
    "        if 'CONFIDENCE' in c:\n",
    "            df_hc[c] = df_hc[c].apply(lambda x: float(x[:-1])/100)\n",
    "            confidnece_columns.append(c)\n",
    "            \n",
    "    df_hc['meet_req'] = df_hc.apply(lambda x: all([x[c] > thrsh for c in confidnece_columns]), axis = 1)\n",
    "    \n",
    "    return set(df_hc[df_hc['meet_req'] == True]['INPUT:text'])\n",
    "    \n",
    "#admit_multi, min_votes, \n",
    "def get_aggr_batch_with_filtering(batch_name, drop_unconfident_thrsh = None, \n",
    "                                  drop_traintest_examples = False, print_output = False, drop_trtest = True):\n",
    "    if batch_name == 'first': \n",
    "        raw_files = ['g1r.tsv','g2r.tsv','g3r.tsv']\n",
    "    elif batch_name == 'fourth':\n",
    "        raw_files = ['g1_4r.tsv','g2_4r.tsv','g3_4r.tsv']\n",
    "    elif batch_name == 'old':\n",
    "        raw_files = ['maybe_old/g1r.tsv','maybe_old/g2r.tsv']\n",
    "    \n",
    "    engaged_texts= set()\n",
    "   \n",
    "    collected_agreements = []\n",
    "\n",
    "    for i, file in enumerate(raw_files):\n",
    "        print(\"=\"*100)\n",
    "        batch_path = \"./directly_from_toloka/\"+file\n",
    "        df_curr_common= pd.read_csv(batch_path, sep = '\\t')\n",
    "        \n",
    "        if drop_trtest == True:\n",
    "            print(\"before dropping trtest ...\", len(df_curr_common))\n",
    "            df_curr_common = df_curr_common[~df_curr_common['INPUT:text'].isin(toloka_trtes_set)]#!!\n",
    "            print(\"after dropping trtest ...\", len(df_curr_common))\n",
    "            \n",
    "        if drop_unconfident_thrsh:\n",
    "            file_agg = re.sub('r','',file)\n",
    "            batch_path_agg = \"./directly_from_toloka/\"+file_agg\n",
    "            df_curr_agg = pd.read_csv(batch_path_agg, sep = '\\t')\n",
    "        \n",
    "            current_high_conf_texts = get_high_conf_texts(df_curr_agg, drop_unconfident_thrsh)\n",
    "            \n",
    "            print(\"before dropping low conf ...\", len(df_curr_common))\n",
    "            df_curr_common = df_curr_common[df_curr_common['INPUT:text'].isin(current_high_conf_texts)]\n",
    "            print(\"after dropping low conf ...\", len(df_curr_common))\n",
    "                \n",
    "        engaged_texts = engaged_texts.union(set(df_curr_common['INPUT:text']))\n",
    "        \n",
    "        columns_to_merge = [c for c in df_curr_common.columns if c == 'INPUT:text' or 'OUTPUT' in c  or c =='ASSIGNMENT:worker_id']\n",
    "        df_curr_common = df_curr_common[columns_to_merge]\n",
    "        \n",
    "        alpha_curr = calc_agreement_within_group_filter(df_curr_common)\n",
    "            \n",
    "        collected_agreements.append(alpha_curr)\n",
    "            \n",
    "    return collected_agreements, engaged_texts\n",
    "\n",
    "aggr, eng_txts = get_aggr_batch_with_filtering('first',drop_unconfident_thrsh = 0.8, print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.39641539683044824, 0.6211801748074177, 0.39857040655455145], 2289)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr, len(eng_txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "before dropping trtest ... 12705\n",
      "after dropping trtest ... 10006\n",
      "before dropping low conf ... 10006\n",
      "after dropping low conf ... 7668\n",
      "====================================================================================================\n",
      "before dropping trtest ... 15009\n",
      "after dropping trtest ... 12250\n",
      "before dropping low conf ... 12250\n",
      "after dropping low conf ... 8540\n",
      "====================================================================================================\n",
      "before dropping trtest ... 16934\n",
      "after dropping trtest ... 13810\n",
      "before dropping low conf ... 13810\n",
      "after dropping low conf ... 8641\n",
      "====================================================================================================\n",
      "before dropping trtest ... 33186\n",
      "after dropping trtest ... 27715\n",
      "before dropping low conf ... 27715\n",
      "after dropping low conf ... 19448\n",
      "====================================================================================================\n",
      "before dropping trtest ... 39363\n",
      "after dropping trtest ... 33894\n",
      "before dropping low conf ... 33894\n",
      "after dropping low conf ... 25467\n",
      "====================================================================================================\n",
      "before dropping trtest ... 27241\n",
      "after dropping trtest ... 23246\n",
      "before dropping low conf ... 23246\n",
      "after dropping low conf ... 16550\n",
      "====================================================================================================\n",
      "before dropping trtest ... 8666\n",
      "after dropping trtest ... 8042\n",
      "before dropping low conf ... 8042\n",
      "after dropping low conf ... 3969\n",
      "====================================================================================================\n",
      "before dropping trtest ... 3898\n",
      "after dropping trtest ... 3678\n",
      "before dropping low conf ... 3678\n",
      "after dropping low conf ... 2203\n"
     ]
    }
   ],
   "source": [
    "def get_filter_stats_from_all_batches(drop_unconfident_thrsh, batches = ['first','fourth','old'],drop_trtest=True):\n",
    "    \n",
    "    collected_agreements_no_pruned = []\n",
    "    \n",
    "    texts_set = set ()\n",
    "    for batch in batches:#\n",
    "        aggr_lst_curr, eng_txts_curr = get_aggr_batch_with_filtering(batch, \n",
    "                                                                     drop_unconfident_thrsh = drop_unconfident_thrsh,\n",
    "                                                                    drop_trtest = drop_trtest)\n",
    "        collected_agreements_no_pruned.extend(aggr_lst_curr)\n",
    "        \n",
    "        texts_set = texts_set.union(eng_txts_curr)\n",
    "    \n",
    "    return np.mean(aggr_lst_curr), texts_set\n",
    "    \n",
    "agrmnt, alltxtx = get_filter_stats_from_all_batches(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4744957067281021, 8694)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agrmnt, len(alltxtx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ЗНАЧЕНИЯ БЕЗ ИСПОЛЬЗВАНИЯ СТАРОГО БАТЧА (НО ПРИ ЭТОМ ТЕРЯЕТСЯ ПОКРТИЕ ЗАЯВЛЕННОЙ ТОЛОКИ)\n",
    "(0.6154409816812249, 0.4973849218904598, 7009, 0.5033888322235552, 7000)\n",
    "\n",
    "ЗНАЧЕНИЯ С ИСПОЛЬЗОВАНИЕМ СТАРОГО БАЧТА (СОГЛАСОВАННОСТЬ ЛУЧШЕ, НО ТОЛОКА ПОКРЫВАЕТСЯ ТОЛЬКО НА 60 ПРОЦ)\n",
    "(0.5300340415816449, 0.42408801155211906, 10130, 0.42859094430194056, 10129)\n",
    "\n",
    "ЗНАЧЕНИЯ С ИСПОЛЬЗОВАНИЕМ СТАРОГО БАЧТА но исходная согласованность подсчитывается через nltk пакет\n",
    "(0.5694888889975732, 0.42408801155211906, 10130, 0.42859094430194056, 10129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_different_filtering_options(batches = ['first','fourth','old'], drop_trtest = True):\n",
    "    data = []\n",
    "    for ds_thrsh_curr in [0.95,0.98]:\n",
    "        agg, engaged_txt = get_filter_stats_from_all_batches(batches = batches, drop_unconfident_thrsh = ds_thrsh_curr, drop_trtest = drop_trtest)\n",
    "        data.append([ds_thrsh_curr, agg, len(engaged_txt)])\n",
    "    df_rep = pd.DataFrame(data = data, columns = ['threshold','agreeement_via_threshold','all_engaged_texts'])\n",
    "    return df_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "before dropping trtest ... 12705\n",
      "after dropping trtest ... 10006\n",
      "before dropping low conf ... 10006\n",
      "after dropping low conf ... 7649\n",
      "====================================================================================================\n",
      "before dropping trtest ... 15009\n",
      "after dropping trtest ... 12250\n",
      "before dropping low conf ... 12250\n",
      "after dropping low conf ... 7797\n",
      "====================================================================================================\n",
      "before dropping trtest ... 16934\n",
      "after dropping trtest ... 13810\n",
      "before dropping low conf ... 13810\n",
      "after dropping low conf ... 6870\n",
      "====================================================================================================\n",
      "before dropping trtest ... 33186\n",
      "after dropping trtest ... 27715\n",
      "before dropping low conf ... 27715\n",
      "after dropping low conf ... 15187\n",
      "====================================================================================================\n",
      "before dropping trtest ... 39363\n",
      "after dropping trtest ... 33894\n",
      "before dropping low conf ... 33894\n",
      "after dropping low conf ... 23329\n",
      "====================================================================================================\n",
      "before dropping trtest ... 27241\n",
      "after dropping trtest ... 23246\n",
      "before dropping low conf ... 23246\n",
      "after dropping low conf ... 14049\n",
      "====================================================================================================\n",
      "before dropping trtest ... 8666\n",
      "after dropping trtest ... 8042\n",
      "before dropping low conf ... 8042\n",
      "after dropping low conf ... 1972\n",
      "====================================================================================================\n",
      "before dropping trtest ... 3898\n",
      "after dropping trtest ... 3678\n",
      "before dropping low conf ... 3678\n",
      "after dropping low conf ... 1296\n",
      "====================================================================================================\n",
      "before dropping trtest ... 12705\n",
      "after dropping trtest ... 10006\n",
      "before dropping low conf ... 10006\n",
      "after dropping low conf ... 7642\n",
      "====================================================================================================\n",
      "before dropping trtest ... 15009\n",
      "after dropping trtest ... 12250\n",
      "before dropping low conf ... 12250\n",
      "after dropping low conf ... 6612\n",
      "====================================================================================================\n",
      "before dropping trtest ... 16934\n",
      "after dropping trtest ... 13810\n",
      "before dropping low conf ... 13810\n",
      "after dropping low conf ... 5897\n",
      "====================================================================================================\n",
      "before dropping trtest ... 33186\n",
      "after dropping trtest ... 27715\n",
      "before dropping low conf ... 27715\n",
      "after dropping low conf ... 12322\n",
      "====================================================================================================\n",
      "before dropping trtest ... 39363\n",
      "after dropping trtest ... 33894\n",
      "before dropping low conf ... 33894\n",
      "after dropping low conf ... 21717\n",
      "====================================================================================================\n",
      "before dropping trtest ... 27241\n",
      "after dropping trtest ... 23246\n",
      "before dropping low conf ... 23246\n",
      "after dropping low conf ... 12303\n",
      "====================================================================================================\n",
      "before dropping trtest ... 8666\n",
      "after dropping trtest ... 8042\n",
      "before dropping low conf ... 8042\n",
      "after dropping low conf ... 1093\n",
      "====================================================================================================\n",
      "before dropping trtest ... 3898\n",
      "after dropping trtest ... 3678\n",
      "before dropping low conf ... 3678\n",
      "after dropping low conf ... 967\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>agreeement_via_threshold</th>\n",
       "      <th>all_engaged_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.608286</td>\n",
       "      <td>7610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.669712</td>\n",
       "      <td>7066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  agreeement_via_threshold  all_engaged_texts\n",
       "0       0.95                  0.608286               7610\n",
       "1       0.98                  0.669712               7066"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_trtes_rep = check_different_filtering_options(drop_trtest=True)\n",
    "df_no_trtes_rep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "before dropping trtest ... 12705\n",
      "after dropping trtest ... 10006\n",
      "before dropping low conf ... 10006\n",
      "after dropping low conf ... 7649\n",
      "====================================================================================================\n",
      "before dropping trtest ... 15009\n",
      "after dropping trtest ... 12250\n",
      "before dropping low conf ... 12250\n",
      "after dropping low conf ... 7797\n",
      "====================================================================================================\n",
      "before dropping trtest ... 16934\n",
      "after dropping trtest ... 13810\n",
      "before dropping low conf ... 13810\n",
      "after dropping low conf ... 6870\n",
      "====================================================================================================\n",
      "before dropping trtest ... 33186\n",
      "after dropping trtest ... 27715\n",
      "before dropping low conf ... 27715\n",
      "after dropping low conf ... 15187\n",
      "====================================================================================================\n",
      "before dropping trtest ... 39363\n",
      "after dropping trtest ... 33894\n",
      "before dropping low conf ... 33894\n",
      "after dropping low conf ... 23329\n",
      "====================================================================================================\n",
      "before dropping trtest ... 27241\n",
      "after dropping trtest ... 23246\n",
      "before dropping low conf ... 23246\n",
      "after dropping low conf ... 14049\n",
      "====================================================================================================\n",
      "before dropping trtest ... 12705\n",
      "after dropping trtest ... 10006\n",
      "before dropping low conf ... 10006\n",
      "after dropping low conf ... 7642\n",
      "====================================================================================================\n",
      "before dropping trtest ... 15009\n",
      "after dropping trtest ... 12250\n",
      "before dropping low conf ... 12250\n",
      "after dropping low conf ... 6612\n",
      "====================================================================================================\n",
      "before dropping trtest ... 16934\n",
      "after dropping trtest ... 13810\n",
      "before dropping low conf ... 13810\n",
      "after dropping low conf ... 5897\n",
      "====================================================================================================\n",
      "before dropping trtest ... 33186\n",
      "after dropping trtest ... 27715\n",
      "before dropping low conf ... 27715\n",
      "after dropping low conf ... 12322\n",
      "====================================================================================================\n",
      "before dropping trtest ... 39363\n",
      "after dropping trtest ... 33894\n",
      "before dropping low conf ... 33894\n",
      "after dropping low conf ... 21717\n",
      "====================================================================================================\n",
      "before dropping trtest ... 27241\n",
      "after dropping trtest ... 23246\n",
      "before dropping low conf ... 23246\n",
      "after dropping low conf ... 12303\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>agreeement_via_threshold</th>\n",
       "      <th>all_engaged_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.640861</td>\n",
       "      <td>6681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.676742</td>\n",
       "      <td>6501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  agreeement_via_threshold  all_engaged_texts\n",
       "0       0.95                  0.640861               6681\n",
       "1       0.98                  0.676742               6501"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = check_different_filtering_options(batches = ['first','fourth'], drop_trtest = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
