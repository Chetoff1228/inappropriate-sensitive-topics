{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DeepPavlov/rubert-base-cased-conversational'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-conversational and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"../../../DATA/toxic/labeled.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = ['text','toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df1['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data = []\n",
    "with open(\"../../../DATA/toxic/dataset.txt\") as f:\n",
    "    for l in f.readlines():\n",
    "        split_line = l.split()\n",
    "        label = split_line[0]\n",
    "        text = ' '.join(split_line[1:])\n",
    "        collected_data.append([text,label])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data = collected_data, columns = ['text','toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>скотина! что сказать</td>\n",
       "      <td>__label__INSULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>__label__NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "      <td>__label__NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "      <td>__label__NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "      <td>__label__NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            toxic\n",
       "0                               скотина! что сказать  __label__INSULT\n",
       "1  я сегодня проезжала по рабочей и между домами ...  __label__NORMAL\n",
       "2  очередной лохотрон. зачем придумывать очередно...  __label__NORMAL\n",
       "3  ретро дежавю ... сложно понять чужое сердце , ...  __label__NORMAL\n",
       "4            а когда мы статус агрогородка получили?  __label__NORMAL"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['toxic'] = df2['toxic'].apply(lambda x: 1 if x != '__label__NORMAL' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>скотина! что сказать</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>я сегодня проезжала по рабочей и между домами ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередной лохотрон. зачем придумывать очередно...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ретро дежавю ... сложно понять чужое сердце , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>а когда мы статус агрогородка получили?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0                               скотина! что сказать      1\n",
       "1  я сегодня проезжала по рабочей и между домами ...      0\n",
       "2  очередной лохотрон. зачем придумывать очередно...      0\n",
       "3  ретро дежавю ... сложно понять чужое сердце , ...      0\n",
       "4            а когда мы статус агрогородка получили?      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262695"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat = pd.concat([df1,df2])\n",
    "df_concat = df_concat.drop_duplicates(subset = ['text'])\n",
    "df_concat = df_concat.sample(frac=1, random_state = 1).reset_index(drop=True)\n",
    "df_concat.dropna(inplace = True)\n",
    "len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_concat['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['toxic'] = df_concat['toxic'].apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>мои помидори без никакого ускорения роста исоз...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>они не понимают что это музыка для души</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>по-моему, это женщина.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>а майонез, можно заменить сметаной</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>кого чьи-то принципы и взгляды не нравится - н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  мои помидори без никакого ускорения роста исоз...      0\n",
       "1            они не понимают что это музыка для души      0\n",
       "2                             по-моему, это женщина.      0\n",
       "3                 а майонез, можно заменить сметаной      0\n",
       "4  кого чьи-то принципы и взгляды не нравится - н...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262695"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ind = int(len(df_concat)*0.8)\n",
    "val_ind = int(len(df_concat)*0.9)\n",
    "\n",
    "df_train = df_concat[:tr_ind]\n",
    "df_val = df_concat[tr_ind:val_ind]\n",
    "df_test = df_concat[val_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81, 0.19)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def get_proprtion(dfc):\n",
    "    return round(len(dfc[dfc['toxic']==0])/len(dfc),2), round(len(dfc[dfc['toxic']==1])/len(dfc),2)\n",
    "\n",
    "get_proprtion(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81, 0.19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_proprtion(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81, 0.19)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_proprtion(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_val) +  len(df_test) == len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanity = pd.concat([df_train,df_val,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_sanity.drop_duplicates(subset = ['text'])) == len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsafeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UnsafeDataset(tokenizer(df_train.text.tolist(),\n",
    "                                        max_length=64,\n",
    "                                        truncation=True,\n",
    "                                        padding='longest'), df_train.toxic.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = UnsafeDataset(tokenizer(df_val.text.tolist(),\n",
    "                                       max_length=64,\n",
    "                                       truncation=True,\n",
    "                                       padding='longest'), df_val.toxic.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = UnsafeDataset(tokenizer(df_test.text.tolist(),\n",
    "                                       max_length=64,\n",
    "                                       truncation=True,\n",
    "                                       padding='longest'), df_test.toxic.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self) -> Tuple[\"torch.device\", int]:\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(device)\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./unsafe/FINAL_VERS',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,            # total # of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=0,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=2500,\n",
    "    eval_steps=2500,\n",
    "    save_steps=2500,\n",
    "    evaluation_strategy='steps',metric_for_best_model = 'f1',greater_is_better = True, load_best_model_at_end = True, report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds_prob = pred.predictions[:,-1]\n",
    "    rauc = roc_auc_score(labels, preds_prob)\n",
    "    \n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"roc_auc\":rauc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics  = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "trainer.add_callback(EarlyStoppingCallback(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in dl:\n",
    "    break\n",
    "with torch.no_grad():\n",
    "    b = {k:v.to(device) for k,v in b.items()}\n",
    "    out = model(**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 210156\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='32840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/32840 35:00 < 56:59, 5.95 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.970117</td>\n",
       "      <td>0.970193</td>\n",
       "      <td>0.970288</td>\n",
       "      <td>0.970117</td>\n",
       "      <td>0.991574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.080221</td>\n",
       "      <td>0.975637</td>\n",
       "      <td>0.975490</td>\n",
       "      <td>0.975451</td>\n",
       "      <td>0.975637</td>\n",
       "      <td>0.994551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.974495</td>\n",
       "      <td>0.974321</td>\n",
       "      <td>0.974284</td>\n",
       "      <td>0.974495</td>\n",
       "      <td>0.993905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.113212</td>\n",
       "      <td>0.974609</td>\n",
       "      <td>0.974503</td>\n",
       "      <td>0.974449</td>\n",
       "      <td>0.974609</td>\n",
       "      <td>0.993022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.088520</td>\n",
       "      <td>0.974951</td>\n",
       "      <td>0.975074</td>\n",
       "      <td>0.975270</td>\n",
       "      <td>0.974951</td>\n",
       "      <td>0.994008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26269\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./unsafe/FINAL_VERS/checkpoint-2500\n",
      "Configuration saved in ./unsafe/FINAL_VERS/checkpoint-2500/config.json\n",
      "Model weights saved in ./unsafe/FINAL_VERS/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./unsafe/FINAL_VERS/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./unsafe/FINAL_VERS/checkpoint-2500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26269\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./unsafe/FINAL_VERS/checkpoint-5000\n",
      "Configuration saved in ./unsafe/FINAL_VERS/checkpoint-5000/config.json\n",
      "Model weights saved in ./unsafe/FINAL_VERS/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./unsafe/FINAL_VERS/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./unsafe/FINAL_VERS/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [unsafe/FINAL_VERS/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26269\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./unsafe/FINAL_VERS/checkpoint-7500\n",
      "Configuration saved in ./unsafe/FINAL_VERS/checkpoint-7500/config.json\n",
      "Model weights saved in ./unsafe/FINAL_VERS/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./unsafe/FINAL_VERS/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./unsafe/FINAL_VERS/checkpoint-7500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26269\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./unsafe/FINAL_VERS/checkpoint-10000\n",
      "Configuration saved in ./unsafe/FINAL_VERS/checkpoint-10000/config.json\n",
      "Model weights saved in ./unsafe/FINAL_VERS/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./unsafe/FINAL_VERS/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./unsafe/FINAL_VERS/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [unsafe/FINAL_VERS/checkpoint-7500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26269\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./unsafe/FINAL_VERS/checkpoint-12500\n",
      "Configuration saved in ./unsafe/FINAL_VERS/checkpoint-12500/config.json\n",
      "Model weights saved in ./unsafe/FINAL_VERS/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./unsafe/FINAL_VERS/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./unsafe/FINAL_VERS/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [unsafe/FINAL_VERS/checkpoint-10000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./unsafe/FINAL_VERS/checkpoint-5000 (score: 0.975490010815315).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.07146613830566406, metrics={'train_runtime': 2101.0369, 'train_samples_per_second': 500.124, 'train_steps_per_second': 15.63, 'total_flos': 1.31548949903616e+16, 'train_loss': 0.07146613830566406, 'epoch': 1.9})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26270\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1642' max='821' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [821/821 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07844485342502594,\n",
       " 'eval_accuracy': 0.974571754853445,\n",
       " 'eval_f1': 0.9744987502580922,\n",
       " 'eval_precision': 0.9744493570735744,\n",
       " 'eval_recall': 0.974571754853445,\n",
       " 'eval_roc_auc': 0.994756509968624,\n",
       " 'eval_runtime': 33.6315,\n",
       " 'eval_samples_per_second': 781.113,\n",
       " 'eval_steps_per_second': 24.412,\n",
       " 'epoch': 1.9}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 26270\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.9744493570735744\n",
      "recall 0.974571754853445\n",
      "fscore_weighted 0.9744987502580922\n",
      "roc_auc 0.9947765325133202\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     21384\n",
      "           1       0.94      0.92      0.93      4886\n",
      "\n",
      "    accuracy                           0.97     26270\n",
      "   macro avg       0.96      0.96      0.96     26270\n",
      "weighted avg       0.97      0.97      0.97     26270\n",
      "\n",
      "   threshold    f1  prec   rec   acc\n",
      "1        0.1  0.97  0.91  0.95  0.97\n",
      "2        0.2  0.97  0.92  0.94  0.97\n",
      "3        0.3  0.97  0.93  0.93  0.97\n",
      "4        0.4  0.97  0.93  0.93  0.97\n",
      "5        0.5  0.97  0.94  0.92  0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9744987502580922"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, roc_auc_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def get_metrics(preds):\n",
    "    preds, labels = preds.predictions, preds.label_ids\n",
    "    #standard round approach    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()    \n",
    "    pr, rec, f, _ = precision_recall_fscore_support(labels, pred_flat, average='weighted')  \n",
    "    \n",
    "    print(\"precision\", pr)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"fscore_weighted\", f)\n",
    "    \n",
    "    #adjust threshold approach\n",
    "    preds_adj = np.array([[float(el1),float(el2)] for el1,el2 in preds])\n",
    "    preds_adj = softmax(preds_adj, axis = 1)\n",
    "    roc_auc = roc_auc_score(labels, preds_adj[:, 1])\n",
    "    print(\"roc_auc\", roc_auc)\n",
    "    \n",
    "    all_metrcis = []\n",
    "    for threshold in [0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1]:\n",
    "        metrcis = []\n",
    "        pred_labels = (preds_adj[:, 1] >= threshold).astype(int)\n",
    "        metrcis.append(threshold)\n",
    "        metrcis.append(round(f1_score(labels, pred_labels, average='weighted'),2))  \n",
    "        metrcis.append(round(precision_score(labels, pred_labels),2))  \n",
    "        metrcis.append(round(recall_score(labels, pred_labels),2))  \n",
    "        metrcis.append(round(accuracy_score(labels, pred_labels),2))  \n",
    "        all_metrcis.append(metrcis)\n",
    "\n",
    "    df_metrics = pd.DataFrame(data = all_metrcis, columns = ['threshold','f1','prec','rec','acc'])\n",
    "    df_metrics = df_metrics.sort_values(by='f1', ascending=False)\n",
    "    \n",
    "    print(classification_report(labels, pred_flat))\n",
    "    \n",
    "    print(df_metrics.head())\n",
    "    \n",
    "    return f\n",
    "\n",
    "get_metrics(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git', '.gitattributes']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = \"../../../toxic_classifier_rus_ok_2ch\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../../../toxic_classifier_rus_ok_2ch\n",
      "Configuration saved in ../../../toxic_classifier_rus_ok_2ch/config.json\n",
      "Model weights saved in ../../../toxic_classifier_rus_ok_2ch/pytorch_model.bin\n",
      "tokenizer config file saved in ../../../toxic_classifier_rus_ok_2ch/tokenizer_config.json\n",
      "Special tokens file saved in ../../../toxic_classifier_rus_ok_2ch/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ../../../toxic_classifier_rus_ok_2ch/tokenizer_config.json\n",
      "Special tokens file saved in ../../../toxic_classifier_rus_ok_2ch/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../../../toxic_classifier_rus_ok_2ch/tokenizer_config.json',\n",
       " '../../../toxic_classifier_rus_ok_2ch/special_tokens_map.json',\n",
       " '../../../toxic_classifier_rus_ok_2ch/vocab.txt',\n",
       " '../../../toxic_classifier_rus_ok_2ch/added_tokens.json')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../../toxic_classifier_rus_ok_2ch/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"DeepPavlov/rubert-base-cased-conversational\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file ../../../toxic_classifier_rus_ok_2ch/pytorch_model.bin\n",
      "Loading PyTorch weights from /notebook/toxic_classifier_rus_ok_2ch/pytorch_model.bin\n",
      "PyTorch checkpoint contains 177,855,490 parameters\n",
      "Loaded 177,854,978 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tf_model = TFBertForSequenceClassification.from_pretrained(path, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../../../toxic_classifier_rus_ok_2ch/config.json\n",
      "Model weights saved in ../../../toxic_classifier_rus_ok_2ch/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "tf_model.save_pretrained(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
