{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Inappropriate_09_top_vs_one_with_multi.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124597"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.75\n",
    "df_unsafe = df[df['inappropriate'] > THRESHOLD]\n",
    "df_safe = df[df['inappropriate'] <= THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs in [1,2,3,4]:\n",
    "    df_unsafe_shuf = df_unsafe.sample(frac=1, random_state = rs).reset_index(drop=True)\n",
    "    split_train = int(len(df_unsafe_shuf) * 0.8)\n",
    "    split_val = int(len(df_unsafe_shuf) * 0.9)\n",
    "    df_unsafe_shuf_train = df_unsafe_shuf[:split_train]\n",
    "    df_unsafe_shuf_val = df_unsafe_shuf[split_train:split_val]\n",
    "    df_unsafe_shuf_test= df_unsafe_shuf[split_val:]\n",
    "\n",
    "    df_safe_shuf = df_safe.sample(frac=1, random_state = rs).reset_index(drop=True)\n",
    "    split_train = int(len(df_safe_shuf) * 0.8)\n",
    "    split_val = int(len(df_safe_shuf) * 0.9)\n",
    "    df_safe_shuf_train = df_safe_shuf[:split_train]\n",
    "    df_safe_shuf_val = df_safe_shuf[split_train:split_val]\n",
    "    df_safe_shuf_test = df_safe_shuf[split_val:]\n",
    "\n",
    "    df_tr = pd.concat([df_unsafe_shuf_train, df_safe_shuf_train])\n",
    "    df_val = pd.concat([df_unsafe_shuf_val, df_safe_shuf_val])\n",
    "    df_test = pd.concat([df_unsafe_shuf_test, df_safe_shuf_test])\n",
    "    \n",
    "    train_path = \"train_randst{}.csv\".format(rs)\n",
    "    val_path = \"val_randst{}.csv\".format(rs)\n",
    "    test_path = \"test_randst{}.csv\".format(rs)\n",
    "    \n",
    "    train_path = os.path.join(\"./splits\",train_path)\n",
    "    val_path = os.path.join(\"./splits\",val_path)\n",
    "    test_path = os.path.join(\"./splits\",test_path)\n",
    "    \n",
    "    df_tr.to_csv(train_path, index = None)\n",
    "    df_val.to_csv(val_path, index = None)\n",
    "    df_test.to_csv(test_path, index = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "# import eli5\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\N.Babakov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_test = pd.read_csv(\"./splits/train_randst1.csv\")\n",
    "df_val_test = pd.read_csv(\"./splits/val_randst1.csv\")\n",
    "df_test_test = pd.read_csv(\"./splits/test_randst1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_val_test = pd.concat([df_tr_test, df_val_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range (1, 3), max_features 150000\n",
      "[[4.29258420e-01 5.70741580e-01]\n",
      " [8.30121961e-03 9.91698780e-01]\n",
      " [6.96101331e-04 9.99303899e-01]\n",
      " [9.60864987e-01 3.91350125e-02]\n",
      " [3.78419188e-01 6.21580812e-01]\n",
      " [6.92670770e-04 9.99307329e-01]\n",
      " [9.55115327e-01 4.48846726e-02]\n",
      " [8.34258001e-01 1.65741999e-01]\n",
      " [3.91414443e-01 6.08585557e-01]\n",
      " [2.23994005e-01 7.76005995e-01]]\n",
      "[1 1 1 0 1 1 0 0 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      9145\n",
      "           1       0.62      0.54      0.58      3315\n",
      "\n",
      "    accuracy                           0.79     12460\n",
      "   macro avg       0.73      0.71      0.72     12460\n",
      "weighted avg       0.78      0.79      0.79     12460\n",
      "\n",
      "(array([0.8403125 , 0.62307692]), array([0.88212138, 0.53755656]), array([0.86070952, 0.57716599]), array([9145, 3315], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7825166783707866, 0.7904494382022472, 0.785272379356356, 0.7977413829644235)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_tfidf(train_val,test, sw = russian_stopwords, ngram_range=(1, 3), max_features=150000, text_label = 'text', debug = False):\n",
    "    \n",
    "    train_val['inappropriate'] = train_val['inappropriate'].apply(round)\n",
    "    test['inappropriate'] = test['inappropriate'].apply(round)\n",
    "    \n",
    "    \n",
    "    text_transformer = TfidfVectorizer(stop_words=sw, ngram_range=ngram_range, lowercase=True, max_features=max_features)\n",
    "    X_train_text = text_transformer.fit_transform(train_val[text_label].tolist())\n",
    "       \n",
    "    X_test_text = text_transformer.transform(test[text_label].tolist())\n",
    "\n",
    "    logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)\n",
    "    \n",
    "    logit.fit(X_train_text, train_val['inappropriate'])\n",
    "    test_preds = logit.predict(X_test_text)\n",
    "    test_predsprob = logit.predict_proba(X_test_text)\n",
    "    \n",
    "    rauc = roc_auc_score(test['inappropriate'].tolist(), test_predsprob[:, 1])\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"ngram_range {}, max_features {}\".format(ngram_range, max_features))\n",
    "        \n",
    "        print(test_predsprob[:10])\n",
    "        \n",
    "        print(test_preds[:10])\n",
    "\n",
    "        print(classification_report(test['inappropriate'].tolist(), test_preds))\n",
    "\n",
    "        print(precision_recall_fscore_support(test['inappropriate'].tolist(),test_preds))\n",
    "        \n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(test['inappropriate'].tolist(),test_preds, average = 'weighted')\n",
    "\n",
    "    return precision, recall, fscore, rauc\n",
    "    \n",
    "check_tfidf(df_tr_val_test, df_test_test, russian_stopwords, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_metric(solve_func):\n",
    "    collected_results = []\n",
    "    for rs in [1,2,3,4]:\n",
    "        df_tr_curr = pd.read_csv(\"./splits/train_randst{}.csv\".format(rs))\n",
    "        df_test_curr = pd.read_csv(\"./splits/test_randst{}.csv\".format(rs))\n",
    "        precision, recall, fscore,rocauc = solve_func(df_tr_curr, df_test_curr)\n",
    "        collected_results.append([precision, recall, fscore, rocauc])\n",
    "    return collected_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = collect_metric(check_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7781494636020585, 0.7863563402889245, 0.7810390893377704],\n",
       " [0.7821026122965965, 0.7902086677367576, 0.7848847011294975],\n",
       " [0.7774199594577708, 0.7865971107544141, 0.7803986299249164],\n",
       " [0.774434350490292, 0.78330658105939, 0.7774869571284697]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7780266 , 0.78661717, 0.78095234, 0.79213747])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidf_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0027341 , 0.00244578, 0.00263564, 0.00202272])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(tfidf_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_SVM(train_val,test, sw = russian_stopwords, ngram_range=(1, 3), max_features=150000, text_label = 'text', debug = False):\n",
    "    \n",
    "    train_val['inappropriate'] = train_val['inappropriate'].apply(round)\n",
    "    test['inappropriate'] = test['inappropriate'].apply(round)\n",
    "    \n",
    "    text_transformer = TfidfVectorizer(stop_words=sw, ngram_range=ngram_range, lowercase=True, max_features=max_features)\n",
    "    X_train_text = text_transformer.fit_transform(train_val[text_label].tolist())\n",
    "       \n",
    "    X_test_text = text_transformer.transform(test[text_label].tolist())\n",
    "\n",
    "    logit = SGDClassifier(loss='log', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)\n",
    "    \n",
    "#     logit = SGDClassifier()\n",
    "    \n",
    "    logit.fit(X_train_text, train_val['inappropriate'])\n",
    "    test_preds = logit.predict(X_test_text)\n",
    "    \n",
    "    test_predsprob = logit.predict_proba(X_test_text)\n",
    "    rauc = roc_auc_score(test['inappropriate'].tolist(), test_predsprob[:, 1])\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"ngram_range {}, max_features {}\".format(ngram_range, max_features))\n",
    "        \n",
    "        print(test_preds[:10])\n",
    "\n",
    "        print(classification_report(test['inappropriate'].tolist(), test_preds))\n",
    "\n",
    "        print(precision_recall_fscore_support(test['inappropriate'].tolist(),test_preds))\n",
    "    \n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(test['inappropriate'].tolist(),test_preds, average = 'weighted')\n",
    "\n",
    "    return precision, recall, fscore, rauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N.Babakov\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\N.Babakov\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\N.Babakov\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\N.Babakov\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "tfidfSVM_data = collect_metric(check_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5386805997490525,\n",
       "  0.7339486356340289,\n",
       "  0.6213339757346165,\n",
       "  0.7190274173344319],\n",
       " [0.5386805997490525,\n",
       "  0.7339486356340289,\n",
       "  0.6213339757346165,\n",
       "  0.718764995336571],\n",
       " [0.5386805997490525,\n",
       "  0.7339486356340289,\n",
       "  0.6213339757346165,\n",
       "  0.720968426399874],\n",
       " [0.5386805997490525,\n",
       "  0.7339486356340289,\n",
       "  0.6213339757346165,\n",
       "  0.7100174249789919]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfSVM_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5386806 , 0.73394864, 0.62133398, 0.71719457])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidfSVM_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.00423022])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(tfidfSVM_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_NB(train_val,test, sw = russian_stopwords, ngram_range=(1, 3), max_features=150000, text_label = 'text', debug = False):\n",
    "    \n",
    "    train_val['inappropriate'] = train_val['inappropriate'].apply(round)\n",
    "    test['inappropriate'] = test['inappropriate'].apply(round)\n",
    "    \n",
    "    text_transformer = TfidfVectorizer(stop_words=sw, ngram_range=ngram_range, lowercase=True, max_features=max_features)\n",
    "    X_train_text = text_transformer.fit_transform(train_val[text_label].tolist())\n",
    "       \n",
    "    X_test_text = text_transformer.transform(test[text_label].tolist())\n",
    "\n",
    "    logit = MultinomialNB()\n",
    "    \n",
    "    logit.fit(X_train_text, train_val['inappropriate'])\n",
    "    test_preds = logit.predict(X_test_text)\n",
    "    \n",
    "    test_predsprob = logit.predict_proba(X_test_text)\n",
    "    rauc = roc_auc_score(test['inappropriate'].tolist(), test_predsprob[:, 1])\n",
    "    \n",
    "    if debug == True:\n",
    "        print(\"ngram_range {}, max_features {}\".format(ngram_range, max_features))\n",
    "        \n",
    "        print(test_preds[:10])\n",
    "\n",
    "        print(classification_report(test['inappropriate'].tolist(), test_preds))\n",
    "\n",
    "        print(precision_recall_fscore_support(test['inappropriate'].tolist(),test_preds))\n",
    "    \n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(test['inappropriate'].tolist(),test_preds, average = 'weighted')\n",
    "\n",
    "    return precision, recall, fscore, rauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfMNB_data = collect_metric(check_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78284437, 0.75234751, 0.66689949, 0.77320666])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tfidfMNB_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00431964, 0.00077475, 0.00111693, 0.00315644])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(tfidfMNB_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 0.75, 0.6666666666666666, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_true = np.array([1,1,0])\n",
    "y_pred = np.array([0,1,0])\n",
    "precision_recall_fscore_support(y_true, y_pred, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
